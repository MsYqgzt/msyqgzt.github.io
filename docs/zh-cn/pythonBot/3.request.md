# requests 模块

## 概念
> python中原生的一款基于网络请求的模块，功能非常强大，简单便捷，效率极高。
>
> 作用：模拟浏览器发送请求。

## 开始使用
- 环境安装（需要先安装python编译环境）
```bash
pip install requests
```
- 使用流程/编码流程
  - 指定URL
  - 基于requests模块发起请求
  - 获取响应对象中的数据值
  - 持久化存储

## 第一个爬虫程序
需求：爬取搜狗首页的页面数据

``` python
#导包
import requests

#step1：指定url
url = 'https://www.sogou.com/'

#step_2：发起请求：使用get方法发起get请求，该方法会返回一个响应对象。参数url表示请求对应的url
response = requests.get(url = url)

#step_3：获取响应数据：通过调用响应对象的text属性，返回响应对象中存储的字符串形式的响应数据（页面源码数据）
page_text = response.text

#step_4：持久化存储
with open('./sogou.html','w',encoding = 'utf-8') as fp:
	fp.write(page_text)
print("爬取数据完毕")
```

## 实战案例
### 1. 爬取搜狗指定词条对应的搜索结果页面（简易网页采集器）

> [!warning|style:callout|label:案例引入|labelVisibility:visible|iconVisibility:visible]
> `UA:User-Agent`（请求载体的身份标识）
> 
> UA检测：门户网站的服务器会监测对应请求的载体身份标识。
> 
> 如果检测到 请求的载体身份标识 为某一款浏览器，则说明该请求为正常请求；
> 
> 如果检测到 请求的载体身份标识 不是浏览器，则表示为不正常请求（爬虫），
> 
> 则服务端有可能拒绝该请求，导致爬取失败。

#### 代码实现 

```python
import requests
#1.指定url
url = 'https://www.sogou.com/web'

#2.UA伪装：将对应的User-Agent封装到字典中
headers = {
	#标识字符串可通过浏览器的开发者选项Network中获取
	'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'
}

#3.处理url携带的参数：封装到字典中
kw = input('输入关键词/Enter a word：')
data = {
	'query': kw
}

#4.对指定的url发起的请求，对应的url携带参数，并在请求过程中处理参数
response = requests.get(url=url,params=data,headers=headers)
page_text = response.text

#6.持久化存储
fileName = kw + '.html'
with open(fileName,'w',encoding = 'utf-8') as fp:
	fp.write(page_text)
print(fileName,'保存成功')
```
<br/>

### 2. 破解百度翻译

> [!tip|style:callout|label:思路|labelVisibility:visible|iconVisibility:visible]
>
> 若此操作**引起页面局部刷新，网页地址不变**，则可以判断此操作发出的是**阿贾克斯（`XHR`）请求**。
>
> 通过抓包工具，获取阿贾克斯（`XHR`）请求，找到带有被翻译字段参数的Post请求链接。
>
> 获取的响应数据（Content-Type）是一组`json`数据。

#### 代码实现 

```python
import requests
import json #引入json库
#1.指定url
post_url = 'https://fanyi.baidu.com/sug'

#2.UA伪装
header = {
	'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'
}

#3.post请求参数处理（同get请求一致）
word = input('Enter a word:')
data = {
	'kw':word
}

#4.请求发送
response = requests.post(url=post_url,data=data,headers=header)

#5.获取响应数据：json()方法返回obj（需确认服务器响应类型为json）
dic_obj = response.json()

#6.持久化存储
fileName = word+'.json'
fp = open(fileName,'w',encoding='utf-8')
json.dump(dic_obj,fp=fp,ensure_ascii=False)

print('操作完成!')
```
 <br/>
### 3. 爬取豆瓣电影分类排行榜`https://movie.douban.com`中的电影详情数据

> [!tip|style:callout|label:思路|labelVisibility:visible|iconVisibility:visible]
> 在豆瓣电影官网点击"排行榜"，进入"喜剧"分类
>
> 所需要获取的，就是这个局部页面内的电影名称等基本信息
>
> 当页面到达最底部时，发现页面获取了新的请求反馈，加载出更多电影信息，且页面地址未变化，因此判断页面发出了阿贾克斯（`XHR`）请求
>
> 在抓包工具中的`Network - XHR`下捕获对应的请求信息，确认`Content-Type`属性为`application/json`后，**获取其`url`，请求类型（GET），以及所有来自表单信息（`Form Data`）的键值对**
>
> 最后分析键值对内的几个关键参数，并自定义使用

#### 代码实现 

```python
import requests
import json
#1.指定url
url = 'https://movie.douban.com/j/chart/top_list'

#2.UA伪装
header = {
	'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'
}

#3.请求参数处理
data = {
	'type': '24',
	'interval_id': '100:90',
	'action': '',
	'start': '0', #表示从第几部电影开始获取
	'limit': '20' #一次获取的信息个数
}

#4.请求发送
response = requests.get(url=url,params=data,headers=header)

#5.获取列表数据：json()方法返回obj
list_data = response.json()

#6.持久化存储
fp = open('./movies.json','w',encoding='utf-8')
json.dump(list_data,fp=fp,ensure_ascii=False)

print('获取结束!')
```

<br/>

### 4. 爬取肯德基餐厅查询`http://www.kfc.com.cn/kfccda/index.aspx`中指定地点的餐厅数

> 根据`Content-Type: text/plain; charset=utf-8`这一属性，返回的结果为text，因此无需调用`json`
> 
> 同样指定`url`为请求包内的链接，获取`Form Data`内的参数作为请求的参数

```python
import requests
#1.指定url
url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword'

#2.UA伪装
header = {
	'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'
}

#3.请求参数处理
kw = input('输入关键字：')
data = {
	'cname': '',
	'pid': '',
	'keyword': kw,
	'pageIndex': '1',#指定页码
	'pageSize': '10'#每页显示的实例个数
}

#4.发送post请求
response = requests.post(url=url,data=data,headers=header)

#5.获取列表数据
list_data = response.text

#6.持久化存储
fileName = kw + '.txt'
with open(fileName,'w',encoding = 'utf-8') as fp:
    fp.write(list_data)

print('获取结束!')
```

<br/>

### 5. 【进阶】爬取国家药品监督管理总局中 基于中华人民共和国化妆品生产许可证相关数据`http://125.35.6.84:81/xk`

### 案例分析

> [!note|style:callout|label:思路|labelVisibility:visible|iconVisibility:visible]
> 此链接的可见内容里并不包含许可证的详细信息，无法确定这个链接是否包含详细信息
>
> 但是可以确定的是，每一个企业名称都对应一个超链接，此链接内包含对应企业的详细数据。
>
> 因此可以尝试，从请求链接中获取页面的标签数据，通过查找`<a>`标签中的`herf`属性获取详细数据的链接

第一步：初步代码，确认对网址请求是否能获取对应页面

```python
import requests
#指定url
url = 'http://125.35.6.84:81/xk'

#UA伪装
header = {
	'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'
}

#发送get请求 + 取列表数据
page_text = requests.get(url=url,headers=header).text

#持久化存储
with open('./HuaZhuangPin.html','w',encoding = 'utf-8') as fp:
    fp.write(page_text)

print('获取结束!')
```

> 验证发现，这种请求方式并不能获取页面内的详细信息，因此不能通过对`url`请求达成预期结果。
>
> 则可以判断，对原`url`请求的内容可能没有企业信息。很有可能时动态加载的信息，需要通过Ajax（`XHR`）请求来获取信息

第二步：刷新页面，确认是否产生`XHR`请求数据

> 验证发现，`XHR`得到了一组`json`数据，包含了页面内列表内的数据
>
> 但我们想的到的是列表内`url`对应的详细数据，于是进一步分析数据内部，发现并没有与`url`关联的数据，但是存在一个`ID`属性
>
> - 通过对详情页`url`的观察发现
>   - `url`的域名都是一样的，只有携带的参数(ID)不一样
>   - ID值可以从首页对应的Ajax请求到的`json`串中获取
>   - 域名和ID值能够拼接出一个完整的企业对应的详情页的`url`

但是这里还有一个坑，还需要确定新得到的详情页数据是否是**来自动态加载的请求**，若是来自动态加载的请求，获取页面数据也就没有意义。

因此我们可以选择用第一步的方法尝试获取，但这样的方法效率极低，可以改成改用抓包工具来查看页面的内容。抓包查看`All`下的数据包，发现没有企业的数据。

所以得出结论，详情页的页面数据也是动态加载出来的！

第三步：刷新详情页面，确认是否产生`XHR`请求数据

> 验证发现，页面的确产生了数据类型为`json`的`HRX`请求，且请求内携带的参数为`id`

------

> 至此就可以得到**完整解决方案**：
> - 通过对原链接发起Ajax请求，获取`json`列表内的`ID`属性
> - 将请求内的`url`与获取到的`ID`拼接，将其作为新的`url`
> - 对新的`url`发起Ajax请求，获取最终的详细数据
> - 优化思路后的**解决方案**：批量获取页面内的`ID`参数，分别与请求的`url`结合，获取对应的企业详细数据

### 代码实现

```python
import requests
import json
#UA伪装
header = {
	'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'
}

#获取首页加载请求的数据
url = 'http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList'
#对应参数的封装
data = {
    'on': 'true',
    'page': '1',#获取的页码
    'pageSize': '15',#每页显示的数量
    'productName': '',
    'conditionType': '1',
    'applyname': '',
    'applysn': ''
}

#发送请求，并获取请求响应数据
json_ids = requests.post(url=url,data=data,headers=header).json()

#遍历获取的json字典中ID数据所在的列表，批量获取并存储id
id_list = [] #用于存储企业id的数组
for dic in json_ids['list']:
    id_list.append(dic['ID']) #将ID值存入存储列表中

#打印查看是否正确获取
#print(id_list)

#--------------------

#获取企业详情数据
post_url = 'http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById'
#遍历id列表，分别与post_url结合发起请求
all_data_list = [] #存储所有企业的详细数据
for id in id_list:
    #对应参数的封装
    data = {
        'id': id
    }
    #发送请求，并获取请求响应数据
    detail_json = requests.post(url=post_url,data=data,headers=header).json()

    #打印查看是否正确获取
    #print(detail_json,'\n----------end----------')
    all_data_list.append(detail_json) #将企业信息存入存储列表中

#持久化存储
fp = open('./allData.json','w',encoding = 'utf-8')
json.dump(all_data_list,fp=fp,ensure_ascii=False)

print('Over!')
```

### 代码优化

上述代码只能获取到其中一页的详细信息，若想要一次性多页（或所有）信息，只需要在页码的绑定数据外添加一个循环，将循环数字转为字符串后动态赋值给绑定数据，最后把只需执行一次的定义类代码移动到前面即可

```python
import requests
import json
#UA伪装
header = {
	'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'
}

id_list = [] #用于存储企业id的数组
all_data_list = [] #存储所有企业的详细数据

#获取首页加载请求的数据
url = 'http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList'

#添加循环，假设只获取前5页
for page in range(1,6):
    page = str(page) #将数字转为字符串
    #对应参数的封装
    data = {
        'on': 'true',
        'page': page,#获取的页码
        'pageSize': '15',#每页显示的数量
        'productName': '',
        'conditionType': '1',
        'applyname': '',
        'applysn': ''
    }
    #发送请求，并获取请求响应数据
    json_ids = requests.post(url=url,data=data,headers=header).json()
    #遍历获取的json字典中ID数据所在的列表，批量获取并存储id
    for dic in json_ids['list']:
        id_list.append(dic['ID']) #将ID值存入存储列表中

#--------------------

#获取企业详情数据
post_url = 'http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById'
#遍历id列表，分别与post_url结合发起请求

for id in id_list:
    #对应参数的封装
    data = {
        'id': id
    }
    #发送请求，并获取请求响应数据
    detail_json = requests.post(url=post_url,data=data,headers=header).json()
	#将企业信息存入存储列表中
    all_data_list.append(detail_json) 

#持久化存储
fp = open('./allData.json','w',encoding = 'utf-8')
json.dump(all_data_list,fp=fp,ensure_ascii=False)

print('Over!')
```

