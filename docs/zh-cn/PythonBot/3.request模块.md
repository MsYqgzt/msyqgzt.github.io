# requests 模块

## 概念
> python中原生的一款基于网络请求的模块，功能非常强大，简单便捷，效率极高。
> 作用：模拟浏览器发送请求。

## 开始使用
- 环境安装（需要先安装python编译环境）
```bash
pip install requests
```
- 使用流程/编码流程
  - 指定URL
  - 基于requests模块发起请求
  - 获取响应对象中的数据值
  - 持久化存储

## 第一个爬虫程序
- 需求：爬取搜狗首页的页面数据

``` python
#导包
import requests

#step1：指定url
url = 'https://www.sogou.com/'

#step_2：发起请求：使用get方法发起get请求，该方法会返回一个响应对象。参数url表示请求对应的url
response = requests.get(url = url)

#step_3：获取响应数据：通过调用响应对象的text属性，返回响应对象中存储的字符串形式的响应数据（页面源码数据）
page_text = response.text

#step_4：持久化存储
with open('./sogou.html','w',encoding = 'utf-8') as fp:
	fp.write(page_text)
print("爬取数据完毕")
```

## 实战案例
1. 爬取搜狗指定词条对应的搜索结果页面（简易网页采集器）

> [!warning|style:flat|label:案例重点|labelVisibility:visible|iconVisibility:visible]
> UA:User-Agent（请求载体的身份标识）
> 
> UA检测：门户网站的服务器会监测对应请求的载体身份标识。
> 
> 如果检测到 请求的载体身份标识 为某一款浏览器，则说明该请求为正常请求；
> 
> 如果检测到 请求的载体身份标识 不是浏览器，则表示为不正常请求（爬虫），
> 
> 则服务端有可能拒绝该请求，导致爬取失败。

```python
import requests
#1.指定url
url = 'https://www.sogou.com/web?'

#2.UA伪装：将对应的User-Agent封装到字典中
headers = {
	#标识字符串可通过浏览器的开发者选项Network中获取
	'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'
}

#3.处理url携带的参数：封装到字典中
kw = input('输入关键词/Enter a word：')
param = {
	'query': kw
}

#4.对指定的url发起的请求，对应的url携带参数，并在请求过程中处理参数
response = requests.get(url=url,params=param,headers=headers)
page_text = response.text

#6.持久化存储
fileName = kw + '.html'
with open(fileName,'w',encoding = 'utf-8') as fp:
	fp.write(page_text)
print(fileName,'保存成功')
```

2. 需求：破解百度翻译

> [!note|style:flat|label:核心思路|labelVisibility:visible|iconVisibility:visible]
> 通过抓包工具，获取阿贾克斯（XHR）请求，找到带有被翻译字段参数的Post请求链接
>
> 获取的响应数据是一组json数据

```python
import requests
import json #引入json库
#1.指定url
post_url = 'https://fanyi.baidu.com/sug'

#2.UA伪装
header = {
	'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'
}

#3.post请求参数处理（同get请求一致）
word = input('Enter a word:')
data = {
	'kw':word
}

#4.请求发送
response = requests.post(url=post_url,data=data,headers=header)

#5.获取响应数据：json()方法返回obj（需确认服务器响应类型为json）
dic_obj = response.json()

#6.持久化存储
fileName = word+'.json'
fp = open(fileName,'w',encoding='utf-8')
json.dump(dic_obj,fp=fp,ensure_ascii=False)

print('操作完成!')
```

3. 需求：爬取豆瓣电影分类排行榜 https://movie.douban.com/ 中的电影详情数据
> [!tip|style:flat|label:思路|labelVisibility:visible|iconVisibility:visible]
> 在豆瓣电影官网点击"排行榜"，进入"喜剧"分类
> 
> 所需要获取的，就是这个局部页面内的电影名称等基本信息
> 
> 当页面到达最底部时，发现页面获取了新的请求反馈，加载出更多电影信息，且页面地址未变化，因此判断页面发出了阿贾克斯（XHR）请求
> 
> 在抓包工具（Network - XHR）中捕获对应的请求信息，确认`Content-Type`属性为`application/json`后
> 
> **获取其url，请求类型（GET），以及所有键值对**，最后分析键值对内的几个关键参数并自定义使用

```python
import requests
import json
#1.指定url
url = 'https://movie.douban.com/j/chart/top_list'

#2.UA伪装
header = {
	'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'
}

#3.get请求参数处理
param = {
	'type': '24',
	'interval_id': '100:90',
	'action': '',
	'start': '0', #表示从第几部电影开始获取
	'limit': '20' #一次获取的信息个数
}

#4.请求发送
response = requests.get(url=url,params=param,headers=header)

#5.获取列表数据：json()方法返回obj
list_data = response.json()

#6.持久化存储
fp = open('./movies.json','w',encoding='utf-8')
json.dump(list_data,fp=fp,ensure_ascii=False)

print('获取结束!')
```

4. 需求：爬取肯德基餐厅查询http://www.kfc.com.cn/kfccda/index.aspx 中指定地点的餐厅数


5. 需求：爬取国家药品监督管理总局中基于中华人民共和国化妆品生产许可证相关数据http://125.3！
